{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This jupyter notebooks provides the code for classifying signals using the Continuous Wavelet Transform and Convolutional Neural Networks.\n",
    "### To get some more background information, please have a look at the accompanying blog-post:\n",
    "### http://ataspinar.com/2018/12/21/a-guide-for-using-the-wavelet-transform-in-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "import keras\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import History \n",
    "history = History()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading the UCI HAR dataset into an numpy ndarray\n",
    "Download dataset from https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train dataset contains 7352 signals, each one of length 128 and 9 components \n",
      "The test dataset contains 2947 signals, each one of length 128 and 9 components \n",
      "The train dataset contains 7352 labels, with the following distribution:\n",
      " Counter({5: 1407, 4: 1374, 3: 1286, 0: 1226, 1: 1073, 2: 986})\n",
      "The test dataset contains 2947 labels, with the following distribution:\n",
      " Counter({5: 537, 4: 532, 0: 496, 3: 491, 1: 471, 2: 420})\n"
     ]
    }
   ],
   "source": [
    "activities_description = {\n",
    "    1: 'walking',\n",
    "    2: 'walking upstairs',\n",
    "    3: 'walking downstairs',\n",
    "    4: 'sitting',\n",
    "    5: 'standing',\n",
    "    6: 'laying'\n",
    "}\n",
    "\n",
    "def read_signals(filename):\n",
    "    with open(filename, 'r') as fp:\n",
    "        data = fp.read().splitlines()\n",
    "        data = map(lambda x: x.strip().split(), data)\n",
    "        data = [list(map(float, line)) for line in data]\n",
    "    return data\n",
    "\n",
    "def read_labels(filename):        \n",
    "    with open(filename, 'r') as fp:\n",
    "        activities = fp.read().splitlines()\n",
    "        activities = list(map(lambda x: int(x)-1, activities))\n",
    "    return activities\n",
    "\n",
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation, :, :]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "\n",
    "DATA_FOLDER = '../../UCI HAR Dataset/'\n",
    "INPUT_FOLDER_TRAIN = DATA_FOLDER+'train/Inertial Signals/'\n",
    "INPUT_FOLDER_TEST = DATA_FOLDER+'test/Inertial Signals/'\n",
    "\n",
    "INPUT_FILES_TRAIN = ['body_acc_x_train.txt', 'body_acc_y_train.txt', 'body_acc_z_train.txt', \n",
    "                     'body_gyro_x_train.txt', 'body_gyro_y_train.txt', 'body_gyro_z_train.txt',\n",
    "                     'total_acc_x_train.txt', 'total_acc_y_train.txt', 'total_acc_z_train.txt']\n",
    "\n",
    "INPUT_FILES_TEST = ['body_acc_x_test.txt', 'body_acc_y_test.txt', 'body_acc_z_test.txt', \n",
    "                     'body_gyro_x_test.txt', 'body_gyro_y_test.txt', 'body_gyro_z_test.txt',\n",
    "                     'total_acc_x_test.txt', 'total_acc_y_test.txt', 'total_acc_z_test.txt']\n",
    "\n",
    "LABELFILE_TRAIN = DATA_FOLDER+'train/y_train.txt'\n",
    "LABELFILE_TEST = DATA_FOLDER+'test/y_test.txt'\n",
    "\n",
    "train_signals, test_signals = [], []\n",
    "\n",
    "for input_file in INPUT_FILES_TRAIN:\n",
    "    signal = read_signals(INPUT_FOLDER_TRAIN + input_file)\n",
    "    train_signals.append(signal)\n",
    "train_signals = np.transpose(train_signals, (1, 2, 0))\n",
    "\n",
    "for input_file in INPUT_FILES_TEST:\n",
    "    signal = read_signals(INPUT_FOLDER_TEST + input_file)\n",
    "    test_signals.append(signal)\n",
    "test_signals = np.transpose(test_signals, (1, 2, 0))\n",
    "\n",
    "train_labels = read_labels(LABELFILE_TRAIN)\n",
    "test_labels = read_labels(LABELFILE_TEST)\n",
    "\n",
    "[no_signals_train, no_steps_train, no_components_train] = np.shape(train_signals)\n",
    "[no_signals_test, no_steps_test, no_components_test] = np.shape(test_signals)\n",
    "no_labels = len(np.unique(train_labels[:]))\n",
    "\n",
    "print(\"The train dataset contains {} signals, each one of length {} and {} components \".format(no_signals_train, no_steps_train, no_components_train))\n",
    "print(\"The test dataset contains {} signals, each one of length {} and {} components \".format(no_signals_test, no_steps_test, no_components_test))\n",
    "print(\"The train dataset contains {} labels, with the following distribution:\\n {}\".format(np.shape(train_labels)[0], Counter(train_labels[:])))\n",
    "print(\"The test dataset contains {} labels, with the following distribution:\\n {}\".format(np.shape(test_labels)[0], Counter(test_labels[:])))\n",
    "\n",
    "uci_har_signals_train, uci_har_labels_train = randomize(train_signals, np.array(train_labels))\n",
    "uci_har_signals_test, uci_har_labels_test = randomize(test_signals, np.array(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Applying a CWT to UCI HAR signals and saving the resulting scaleogram into an numpy ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "0\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "scales = range(1,128)\n",
    "waveletname = 'morl'\n",
    "train_size = 2000\n",
    "train_data_cwt = np.ndarray(shape=(train_size, 127, 127, 9))\n",
    "\n",
    "for ii in range(0,train_size):\n",
    "    if ii % 1000 == 0:\n",
    "        print(ii)\n",
    "    for jj in range(0,9):\n",
    "        signal = uci_har_signals_train[ii, :, jj]\n",
    "        coeff, freq = pywt.cwt(signal, scales, waveletname, 1)\n",
    "        coeff_ = coeff[:,:127]\n",
    "        train_data_cwt[ii, :, :, jj] = coeff_\n",
    "\n",
    "test_size = 200\n",
    "test_data_cwt = np.ndarray(shape=(test_size, 127, 127, 9))\n",
    "for ii in range(0,test_size):\n",
    "    if ii % 100 == 0:\n",
    "        print(ii)\n",
    "    for jj in range(0,9):\n",
    "        signal = uci_har_signals_test[ii, :, jj]\n",
    "        coeff, freq = pywt.cwt(signal, scales, waveletname, 1)\n",
    "        coeff_ = coeff[:,:127]\n",
    "        test_data_cwt[ii, :, :, jj] = coeff_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (2000, 127, 127, 9)\n",
      "2000 train samples\n",
      "200 test samples\n"
     ]
    }
   ],
   "source": [
    "x_train = train_data_cwt\n",
    "y_train = list(uci_har_labels_train[:train_size])\n",
    "x_test = test_data_cwt\n",
    "y_test = list(uci_har_labels_test[:test_size])\n",
    "img_x = 127\n",
    "img_y = 127\n",
    "img_z = 9\n",
    "num_classes = 6\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 4\n",
    "\n",
    "# reshape the data into a 4D tensor - (sample_number, x_img_size, y_img_size, num_channels)\n",
    "# because the MNIST is greyscale, we only have a single channel - RGB colour images would have 3\n",
    "input_shape = (img_x, img_y, img_z)\n",
    "\n",
    "# convert the data to the right type\n",
    "#x_train = x_train.reshape(x_train.shape[0], img_x, img_y, img_z)\n",
    "#x_test = x_test.reshape(x_test.shape[0], img_x, img_y, img_z)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices - this is for use in the\n",
    "# categorical_crossentropy loss below\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "125/125 [==============================] - 84s 668ms/step - loss: 0.8514 - accuracy: 0.8035 - val_loss: 0.5188 - val_accuracy: 0.8800\n",
      "Epoch 2/4\n",
      "125/125 [==============================] - 102s 815ms/step - loss: 0.1864 - accuracy: 0.9290 - val_loss: 0.6394 - val_accuracy: 0.8800\n",
      "Epoch 3/4\n",
      "125/125 [==============================] - 96s 767ms/step - loss: 0.1668 - accuracy: 0.9385 - val_loss: 0.4799 - val_accuracy: 0.8800\n",
      "Epoch 4/4\n",
      "125/125 [==============================] - 92s 739ms/step - loss: 0.1119 - accuracy: 0.9520 - val_loss: 0.4477 - val_accuracy: 0.9050\n",
      "Train loss: 0.09162799268960953, Train accuracy: 0.9629999995231628\n",
      "Test loss: 0.44771477580070496, Test accuracy: 0.9049999713897705\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1), activation='relu', input_shape=input_shape)) \n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "              optimizer=keras.optimizers.Adam(), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, \n",
    "          epochs=epochs, verbose=1, \n",
    "          validation_data=(x_test, y_test), \n",
    "          callbacks=[history])\n",
    "\n",
    "train_score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print('Train loss: {}, Train accuracy: {}'.format(train_score[0], train_score[1]))\n",
    "test_score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss: {}, Test accuracy: {}'.format(test_score[0], test_score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(figsize=(12,6), ncols=2)\n",
    "axarr[0].plot(range(1, epochs+1), history.history['accuracy'], label='train score')\n",
    "axarr[0].plot(range(1, epochs+1), history.history['val_accuracy'], label='test score')\n",
    "axarr[0].set_xlabel('Number of Epochs', fontsize=18)\n",
    "axarr[0].set_ylabel('Accuracy', fontsize=18)\n",
    "axarr[0].set_ylim([0,1])\n",
    "axarr[1].plot(range(1, epochs+1), history.history['accuracy'], label='train score')\n",
    "axarr[1].plot(range(1, epochs+1), history.history['val_accuracy'], label='test score')\n",
    "axarr[1].set_xlabel('Number of Epochs', fontsize=18)\n",
    "axarr[1].set_ylabel('Accuracy', fontsize=18)\n",
    "axarr[1].set_ylim([0.8,1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
