{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This jupyter notebooks provides the code for classifying signals using the Discrete Wavelet Transform.\n",
    "### To get some more background information, please have a look at the accompanying blog-post:\n",
    "### http://ataspinar.com/2018/12/21/a-guide-for-using-the-wavelet-transform-in-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import perf_counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import keras\n",
    "from keras.layers import Conv1D, BatchNormalization, Dense, Flatten, Activation\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import History \n",
    "history = History()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_classify(X_train, Y_train, X_test, Y_test, no_classifiers = 6, verbose = True):\n",
    "    \"\"\"\n",
    "    This method, takes as input the X, Y matrices of the Train and Test set.\n",
    "    And fits them on all of the Classifiers specified in the dict_classifier.\n",
    "    Usually, the SVM, Random Forest and Gradient Boosting Classifier take quiet some time to train. \n",
    "    So it is best to train them on a smaller dataset first and \n",
    "    decide whether you want to comment them out or not based on the test accuracy score.\n",
    "    \"\"\"\n",
    "    \n",
    "    dict_models = {}\n",
    "    for classifier_name, classifier in list(dict_classifiers.items())[:no_classifiers]:\n",
    "        t_start = perf_counter()\n",
    "        classifier.fit(X_train, Y_train)\n",
    "        t_end = perf_counter()\n",
    "        \n",
    "        t_diff = t_end - t_start\n",
    "        train_score = classifier.score(X_train, Y_train)\n",
    "        test_score = classifier.score(X_test, Y_test)\n",
    "        \n",
    "        dict_models[classifier_name] = {'model': classifier, 'train_score': train_score, 'test_score': test_score, 'train_time': t_diff}\n",
    "        if verbose:\n",
    "            print(\"trained {c} in {f:.2f} s\".format(c=classifier_name, f=t_diff))\n",
    "    return dict_models\n",
    "\n",
    "    \n",
    "def display_dict_models(dict_models, sort_by='test_score'):\n",
    "    cls = [key for key in dict_models.keys()]\n",
    "    test_s = [dict_models[key]['test_score'] for key in cls]\n",
    "    training_s = [dict_models[key]['train_score'] for key in cls]\n",
    "    training_t = [dict_models[key]['train_time'] for key in cls]\n",
    "    \n",
    "    df_ = pd.DataFrame(data=np.zeros(shape=(len(cls),4)), columns = ['classifier', 'train_score', 'test_score', 'train_time'])\n",
    "    for ii in range(0,len(cls)):\n",
    "        df_.loc[ii, 'classifier'] = cls[ii]\n",
    "        df_.loc[ii, 'train_score'] = training_s[ii]\n",
    "        df_.loc[ii, 'test_score'] = test_s[ii]\n",
    "        df_.loc[ii, 'train_time'] = training_t[ii]\n",
    "    \n",
    "    display(df_.sort_values(by=sort_by, ascending=False))\n",
    "    \n",
    "def get_features(list_values):\n",
    "    entropy = calculate_entropy(list_values)\n",
    "    crossings = calculate_crossings(list_values)\n",
    "    statistics = calculate_statistics(list_values)\n",
    "    return [entropy] + crossings + statistics\n",
    "\n",
    "def get_uci_har_features(dataset, labels, waveletname, compress_lim=None):\n",
    "    uci_har_features = []\n",
    "    # Take the DWT of each component and concat them end-to-end\n",
    "    for signal_no in range(0, len(dataset)):\n",
    "        features = []\n",
    "        for signal_comp in range(0,dataset.shape[2]):\n",
    "            signal = dataset[signal_no, :, signal_comp]\n",
    "            list_coeff = pywt.wavedec(signal, waveletname, mode='per')\n",
    "            # convert the wavelet decomposition to array\n",
    "            comp_dwt=[]\n",
    "            for coeff in list_coeff:\n",
    "                comp_dwt.extend(coeff)\n",
    "            features.append(comp_dwt[:compress_lim])\n",
    "        uci_har_features.append(features)\n",
    "    X = np.array(uci_har_features).transpose(0,2,1)\n",
    "    Y = labels\n",
    "    print(type(labels))\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading the UCI HAR dataset\n",
    "download dataset from https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train dataset contains 7352 signals, each one of length 128 and 9 components \n",
      "The test dataset contains 7352 signals, each one of length 128 and 9 components \n",
      "The train dataset contains 7352 labels, with the following distribution:\n",
      " Counter({5: 1407, 4: 1374, 3: 1286, 0: 1226, 1: 1073, 2: 986})\n",
      "The test dataset contains 2947 labels, with the following distribution:\n",
      " Counter({5: 537, 4: 532, 0: 496, 3: 491, 1: 471, 2: 420})\n"
     ]
    }
   ],
   "source": [
    "activities_description = {\n",
    "    1: 'walking',\n",
    "    2: 'walking upstairs',\n",
    "    3: 'walking downstairs',\n",
    "    4: 'sitting',\n",
    "    5: 'standing',\n",
    "    6: 'laying'\n",
    "}\n",
    "\n",
    "def read_signals(filename):\n",
    "    with open(filename, 'r') as fp:\n",
    "        data = fp.read().splitlines()\n",
    "        data = map(lambda x: x.rstrip().lstrip().split(), data)\n",
    "        data = [list(map(float, line)) for line in data]\n",
    "    return data\n",
    "\n",
    "def read_labels(filename):        \n",
    "    with open(filename, 'r') as fp:\n",
    "        activities = fp.read().splitlines()\n",
    "        activities = list(map(lambda x: int(x)-1, activities))\n",
    "    return activities\n",
    "\n",
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation, :, :]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "\n",
    "DATA_FOLDER = '../datasets/UCI HAR Dataset/'\n",
    "INPUT_FOLDER_TRAIN = DATA_FOLDER+'train/Inertial Signals/'\n",
    "INPUT_FOLDER_TEST = DATA_FOLDER+'test/Inertial Signals/'\n",
    "\n",
    "INPUT_FILES_TRAIN = ['body_acc_x_train.txt', 'body_acc_y_train.txt', 'body_acc_z_train.txt', \n",
    "                     'body_gyro_x_train.txt', 'body_gyro_y_train.txt', 'body_gyro_z_train.txt',\n",
    "                     'total_acc_x_train.txt', 'total_acc_y_train.txt', 'total_acc_z_train.txt']\n",
    "\n",
    "INPUT_FILES_TEST = ['body_acc_x_test.txt', 'body_acc_y_test.txt', 'body_acc_z_test.txt', \n",
    "                     'body_gyro_x_test.txt', 'body_gyro_y_test.txt', 'body_gyro_z_test.txt',\n",
    "                     'total_acc_x_test.txt', 'total_acc_y_test.txt', 'total_acc_z_test.txt']\n",
    "\n",
    "LABELFILE_TRAIN = DATA_FOLDER+'train/y_train.txt'\n",
    "LABELFILE_TEST = DATA_FOLDER+'test/y_test.txt'\n",
    "\n",
    "train_signals, test_signals = [], []\n",
    "\n",
    "for input_file in INPUT_FILES_TRAIN:\n",
    "    signal = read_signals(INPUT_FOLDER_TRAIN + input_file)\n",
    "    train_signals.append(signal)\n",
    "train_signals = np.transpose(np.array(train_signals), (1, 2, 0))\n",
    "\n",
    "for input_file in INPUT_FILES_TEST:\n",
    "    signal = read_signals(INPUT_FOLDER_TEST + input_file)\n",
    "    test_signals.append(signal)\n",
    "test_signals = np.transpose(np.array(test_signals), (1, 2, 0))\n",
    "\n",
    "train_labels = read_labels(LABELFILE_TRAIN)\n",
    "test_labels = read_labels(LABELFILE_TEST)\n",
    "\n",
    "[no_signals_train, no_steps_train, no_components_train] = np.shape(train_signals)\n",
    "[no_signals_test, no_steps_test, no_components_test] = np.shape(train_signals)\n",
    "no_labels = len(np.unique(train_labels[:]))\n",
    "\n",
    "print(\"The train dataset contains {} signals, each one of length {} and {} components \".format(no_signals_train, no_steps_train, no_components_train))\n",
    "print(\"The test dataset contains {} signals, each one of length {} and {} components \".format(no_signals_test, no_steps_test, no_components_test))\n",
    "print(\"The train dataset contains {} labels, with the following distribution:\\n {}\".format(np.shape(train_labels)[0], Counter(train_labels[:])))\n",
    "print(\"The test dataset contains {} labels, with the following distribution:\\n {}\".format(np.shape(test_labels)[0], Counter(test_labels[:])))\n",
    "\n",
    "uci_har_signals_train, uci_har_labels_train = randomize(train_signals, np.array(train_labels))\n",
    "uci_har_signals_test, uci_har_labels_test = randomize(test_signals, np.array(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Generating features for the UCI-HAR features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Training data shape: (7352, 128, 9)\n",
      "Test data shape: (2947, 128, 9)\n"
     ]
    }
   ],
   "source": [
    "waveletname = 'rbio3.1'\n",
    "x_train, y_train = get_uci_har_features(uci_har_signals_train, uci_har_labels_train, waveletname)\n",
    "x_test, y_test = get_uci_har_features(uci_har_signals_test, uci_har_labels_test, waveletname)\n",
    "print ('Training data shape: {}'.format(x_train.shape))\n",
    "print ('Test data shape: {}'.format(x_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classifying the train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape: (128, 9)\n",
      "x_train shape: (7352, 128, 9)\n",
      "7352 train samples\n",
      "2947 test samples\n"
     ]
    }
   ],
   "source": [
    "num_classes = 6\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 16\n",
    "\n",
    "input_shape = np.shape(x_train)[1:]\n",
    "print('input_shape: {}'.format(input_shape))\n",
    "\n",
    "# convert the data to the right type\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices - this is for use in the\n",
    "# categorical_crossentropy loss below\n",
    "y_train = list(y_train)\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = list(y_test)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "normalization (Normalization (None, 128, 9)            19        \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 61, 16)            1024      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 61, 16)            64        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 61, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 59, 32)            1568      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 59, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 59, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1888)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               188900    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 40)                4040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 40)                160       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 246       \n",
      "=================================================================\n",
      "Total params: 196,549\n",
      "Trainable params: 196,154\n",
      "Non-trainable params: 395\n",
      "_________________________________________________________________\n",
      "Epoch 1/16\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 0.6924 - accuracy: 0.8546 - val_loss: 0.5969 - val_accuracy: 0.8680\n",
      "Epoch 2/16\n",
      "460/460 [==============================] - 3s 6ms/step - loss: 0.4203 - accuracy: 0.9314 - val_loss: 0.4693 - val_accuracy: 0.9046\n",
      "Epoch 3/16\n",
      "460/460 [==============================] - 3s 6ms/step - loss: 0.3485 - accuracy: 0.9426 - val_loss: 0.4203 - val_accuracy: 0.9104\n",
      "Epoch 4/16\n",
      "460/460 [==============================] - 3s 6ms/step - loss: 0.3042 - accuracy: 0.9470 - val_loss: 0.4515 - val_accuracy: 0.9040\n",
      "Epoch 5/16\n",
      "460/460 [==============================] - 3s 5ms/step - loss: 0.2899 - accuracy: 0.9484 - val_loss: 0.4047 - val_accuracy: 0.9179\n",
      "Epoch 6/16\n",
      "460/460 [==============================] - 3s 6ms/step - loss: 0.2676 - accuracy: 0.9516 - val_loss: 0.3902 - val_accuracy: 0.9192\n",
      "Epoch 7/16\n",
      "460/460 [==============================] - 3s 7ms/step - loss: 0.2507 - accuracy: 0.9543 - val_loss: 0.4126 - val_accuracy: 0.9118\n",
      "Epoch 8/16\n",
      "460/460 [==============================] - 3s 6ms/step - loss: 0.2615 - accuracy: 0.9531 - val_loss: 0.4725 - val_accuracy: 0.8768\n",
      "Epoch 9/16\n",
      "460/460 [==============================] - 3s 6ms/step - loss: 0.2452 - accuracy: 0.9558 - val_loss: 0.3406 - val_accuracy: 0.9291\n",
      "Epoch 10/16\n",
      "460/460 [==============================] - 3s 7ms/step - loss: 0.2411 - accuracy: 0.9543 - val_loss: 0.3566 - val_accuracy: 0.9182\n",
      "Epoch 11/16\n",
      "460/460 [==============================] - 3s 7ms/step - loss: 0.2400 - accuracy: 0.9558 - val_loss: 0.3759 - val_accuracy: 0.9247\n",
      "Epoch 12/16\n",
      "460/460 [==============================] - 3s 6ms/step - loss: 0.2221 - accuracy: 0.9611 - val_loss: 0.3420 - val_accuracy: 0.9189\n",
      "Epoch 13/16\n",
      "460/460 [==============================] - 3s 6ms/step - loss: 0.2272 - accuracy: 0.9589 - val_loss: 0.4071 - val_accuracy: 0.9294\n",
      "Epoch 14/16\n",
      "460/460 [==============================] - 3s 5ms/step - loss: 0.2334 - accuracy: 0.9608 - val_loss: 0.3561 - val_accuracy: 0.9359\n",
      "Epoch 15/16\n",
      "460/460 [==============================] - 3s 6ms/step - loss: 0.1963 - accuracy: 0.9664 - val_loss: 0.3468 - val_accuracy: 0.9328\n",
      "Epoch 16/16\n",
      "460/460 [==============================] - 3s 6ms/step - loss: 0.2027 - accuracy: 0.9683 - val_loss: 0.3873 - val_accuracy: 0.9301\n",
      "Train loss: 0.20433852076530457, Train accuracy: 0.9752448201179504\n",
      "Test loss: 0.3873322010040283, Test accuracy: 0.9300984144210815\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "normalizer = preprocessing.Normalization()\n",
    "normalizer.adapt(x_train)\n",
    "\n",
    "model.add(keras.Input(shape=input_shape))\n",
    "model.add(normalizer)\n",
    "model.add(Conv1D(16, kernel_size=7, strides=2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(32, 3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, kernel_regularizer=keras.regularizers.l1_l2(l1=5e-5,l2=1e-5)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(40, kernel_regularizer=keras.regularizers.l1_l2(l1=5e-5,l2=1e-5)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "              optimizer=keras.optimizers.Adam(), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, \n",
    "          epochs=epochs, verbose=1, \n",
    "          validation_data=(x_test, y_test), \n",
    "          callbacks=[history])\n",
    "\n",
    "train_score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print('Train loss: {}, Train accuracy: {}'.format(train_score[0], train_score[1]))\n",
    "test_score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss: {}, Test accuracy: {}'.format(test_score[0], test_score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(figsize=(14,7), ncols=2)\n",
    "axarr[0].plot(range(1, epochs+1), history.history['accuracy'], label='train accuracy')\n",
    "axarr[0].plot(range(1, epochs+1), history.history['val_accuracy'], label='test accuracy')\n",
    "axarr[0].set_xlabel('Number of Epochs', fontsize=18)\n",
    "axarr[0].set_ylabel('Accuracy', fontsize=18)\n",
    "axarr[0].set_ylim([0.5,1])\n",
    "axarr[0].legend()\n",
    "\n",
    "axarr[1].plot(range(1, epochs+1), history.history['loss'], label='train loss')\n",
    "axarr[1].plot(range(1, epochs+1), history.history['val_loss'], label='test loss')\n",
    "axarr[1].set_xlabel('Number of Epochs', fontsize=18)\n",
    "axarr[1].set_ylabel('Loss', fontsize=18)\n",
    "axarr[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
