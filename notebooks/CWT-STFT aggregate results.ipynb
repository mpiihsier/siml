{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This jupyter notebooks provides the code for classifying signals using DCT and various DWTs, with various lengths of truncation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'scipy_signal' from 'scipy' (/Applications/anaconda3/lib/python3.8/site-packages/scipy/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a47be9f7f17c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy_signal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'scipy_signal' from 'scipy' (/Applications/anaconda3/lib/python3.8/site-packages/scipy/__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from time import perf_counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "from collections import defaultdict, Counter\n",
    "from scipy import signal as scipy_signal\n",
    "\n",
    "import keras\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dense, Flatten, Activation, Dropout\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import History, EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import pdb\n",
    "history = History()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading the UCI HAR dataset\n",
    "Download the dataset from https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_description = {\n",
    "    0: 'walking',\n",
    "    1: 'walking upstairs',\n",
    "    2: 'walking downstairs',\n",
    "    3: 'sitting',\n",
    "    4: 'standing',\n",
    "    5: 'laying'\n",
    "}\n",
    "\n",
    "def read_signals(filename):\n",
    "    with open(filename, 'r') as fp:\n",
    "        data = fp.read().splitlines()\n",
    "        data = map(lambda x: x.rstrip().lstrip().split(), data)\n",
    "        data = [list(map(float, line)) for line in data]\n",
    "    return data\n",
    "\n",
    "def read_labels(filename):        \n",
    "    with open(filename, 'r') as fp:\n",
    "        activities = fp.read().splitlines()\n",
    "        activities = list(map(lambda x: int(x)-1, activities))\n",
    "    return activities\n",
    "\n",
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation, :, :]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "\n",
    "DATA_FOLDER = '../datasets/UCI HAR Dataset/'\n",
    "INPUT_FOLDER_TRAIN = DATA_FOLDER+'train/Inertial Signals/'\n",
    "INPUT_FOLDER_TEST = DATA_FOLDER+'test/Inertial Signals/'\n",
    "\n",
    "INPUT_FILES_TRAIN = ['body_acc_x_train.txt', 'body_acc_y_train.txt', 'body_acc_z_train.txt', \n",
    "                     'body_gyro_x_train.txt', 'body_gyro_y_train.txt', 'body_gyro_z_train.txt',\n",
    "                     'total_acc_x_train.txt', 'total_acc_y_train.txt', 'total_acc_z_train.txt']\n",
    "\n",
    "INPUT_FILES_TEST = ['body_acc_x_test.txt', 'body_acc_y_test.txt', 'body_acc_z_test.txt', \n",
    "                     'body_gyro_x_test.txt', 'body_gyro_y_test.txt', 'body_gyro_z_test.txt',\n",
    "                     'total_acc_x_test.txt', 'total_acc_y_test.txt', 'total_acc_z_test.txt']\n",
    "\n",
    "LABELFILE_TRAIN = DATA_FOLDER+'train/y_train.txt'\n",
    "LABELFILE_TEST = DATA_FOLDER+'test/y_test.txt'\n",
    "\n",
    "train_signals, test_signals = [], []\n",
    "\n",
    "for input_file in INPUT_FILES_TRAIN:\n",
    "    signal = read_signals(INPUT_FOLDER_TRAIN + input_file)\n",
    "    train_signals.append(signal)\n",
    "train_signals = np.transpose(np.array(train_signals), (1, 2, 0))\n",
    "\n",
    "for input_file in INPUT_FILES_TEST:\n",
    "    signal = read_signals(INPUT_FOLDER_TEST + input_file)\n",
    "    test_signals.append(signal)\n",
    "test_signals = np.transpose(np.array(test_signals), (1, 2, 0))\n",
    "\n",
    "train_labels = read_labels(LABELFILE_TRAIN)\n",
    "test_labels = read_labels(LABELFILE_TEST)\n",
    "\n",
    "[no_signals_train, no_steps_train, no_components_train] = np.shape(train_signals)\n",
    "[no_signals_test, no_steps_test, no_components_test] = np.shape(train_signals)\n",
    "no_labels = len(np.unique(train_labels[:]))\n",
    "\n",
    "print(\"The train dataset contains {} signals, each one of length {} and {} components \".format(no_signals_train, no_steps_train, no_components_train))\n",
    "print(\"The test dataset contains {} signals, each one of length {} and {} components \".format(no_signals_test, no_steps_test, no_components_test))\n",
    "print(\"The train dataset contains {} labels, with the following distribution:\\n {}\".format(np.shape(train_labels)[0], Counter(train_labels[:])))\n",
    "print(\"The test dataset contains {} labels, with the following distribution:\\n {}\".format(np.shape(test_labels)[0], Counter(test_labels[:])))\n",
    "\n",
    "uci_har_signals_train, uci_har_labels_train = randomize(train_signals, np.array(train_labels))\n",
    "uci_har_signals_test, uci_har_labels_test = randomize(test_signals, np.array(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's assess the signals' frequency/scale domain properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import log10, absolute as abs\n",
    "from scipy.fftpack import dct,fft\n",
    "\n",
    "def get_spectrogram(sig, M, noverlap, fs=1.0, windowname = ('hamming'), \n",
    "                     ax=None, cmap = plt.cm.seismic):\n",
    "    # get the window taps\n",
    "    win = scipy_signal.get_window(windowname,M,False)\n",
    "    # prefix/suffix\n",
    "    pref = sig[-int(M/2):]*win[0:int(M/2)]\n",
    "    suf = sig[0:int(M/2)]*win[-int(M/2):]\n",
    "\n",
    "    sig = np.concatenate((pref, sig))\n",
    "    sig = np.concatenate((sig,suf))\n",
    "    f, t, mag = scipy_signal.spectrogram(sig, fs, window=win, nperseg=M, noverlap=noverlap,\n",
    "                                  scaling='spectrum', mode='magnitude')\n",
    "    if ax:\n",
    "        ax.pcolormesh(t, np.flip(f), np.flip(mag,axis=0), shading='nearest',cmap=cmap)\n",
    "    return mag\n",
    "\n",
    "def get_cwt(sig, wavelet, ax=None, cmap=plt.cm.seismic):\n",
    "    voices_per_octave = 8\n",
    "    max_scale = 8\n",
    "    begin_scale = 1/(2**1)\n",
    "    scales = list(range(1,int((max_scale/begin_scale)*voices_per_octave)))\n",
    "    scales = [begin_scale*float(x) for x in scales]\n",
    "    coef,f = pywt.cwt(sig,scales,wavelet)\n",
    "    if ax:\n",
    "        ax.matshow(np.flip(coef,axis=0), cmap=cmap, extent=[0,len(sig)-1,scales[-1],scales[0]], aspect='auto')\n",
    "        ax.xaxis.set_ticks_position('bottom')\n",
    "    return coef\n",
    "    \n",
    "# Randomly choose 10 signals from the training set\n",
    "sample_set = np.random.randint(len(train_signals),size=1)\n",
    "# Choose 3 random components to analyze\n",
    "rand_comp = np.random.randint(9,size=3)\n",
    "\n",
    "wavelets=['gaus1','morl','mexh']\n",
    "#wavelets=['gaus1']\n",
    "\n",
    "# Parameters for spectrogram\n",
    "win = 'hann'\n",
    "M = 32\n",
    "noverlap = M-2\n",
    "\n",
    "for i,sig in enumerate(train_signals[sample_set]):\n",
    "    ## Now we plot\n",
    "    for c in rand_comp:\n",
    "        fig, ax = plt.subplots(figsize=(18,18), nrows=2,ncols=2)\n",
    "        fig.suptitle('Train sample {} ({}), {}'.format(sample_set[i],\n",
    "            activities_description[train_labels[sample_set[i]]],INPUT_FILES_TRAIN[c][:11]),\n",
    "                     fontsize=24)\n",
    "\n",
    "        ax[0][0].set_title('Spectrogram ({})'.format(win), fontsize=18)\n",
    "        get_spectrogram(sig[:,c], M, noverlap, ax=ax[0][0])\n",
    "        ax[0][0].set_ylabel('Frequency', fontsize=16)\n",
    "        ax[0][0].set_xlabel('Time', fontsize=16)\n",
    "\n",
    "        curr_ax=1\n",
    "        for w in wavelets:\n",
    "            px,py = int(curr_ax/2), curr_ax-(2*int(curr_ax/2))\n",
    "            get_cwt(sig[:,c], w, ax=ax[px][py])\n",
    "            curr_ax += 1\n",
    "            ax[px][py].set_title('Scalogram ({})'.format(w), fontsize=18)\n",
    "            ax[px][py].set_ylabel('Scales', fontsize=16)\n",
    "            ax[px][py].set_xlabel('Translation', fontsize=16)\n",
    "\n",
    "        fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we define the neural networks that we intend to use for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN: Classifier for 2D inputs for UCI HAR\n",
    "def NN (x_train, y_train, x_test, y_test, print_nn=False, verbosity=0):\n",
    "\n",
    "    num_classes = 6\n",
    "\n",
    "    batch_size = 16\n",
    "    epochs = 32\n",
    "\n",
    "    input_shape = np.shape(x_train)[1:]\n",
    "\n",
    "    # convert the data to the right type\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "\n",
    "    # convert class vectors to binary class matrices - this is for use in the\n",
    "    # categorical_crossentropy loss below\n",
    "    y_train = list(y_train)\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = list(y_test)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.2, input_shape=input_shape)) \n",
    "    model.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1), activation='relu', input_shape=input_shape))\n",
    "    model.add(Dropout(0.2)) \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "    model.add(Dropout(0.2)) \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1000, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    if print_nn:\n",
    "        model.summary()\n",
    "        return\n",
    "    \n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=keras.optimizers.Adam(), \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', verbose=0, patience=4)\n",
    "    t_start = perf_counter()\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, \n",
    "              epochs=epochs, verbose=verbosity, \n",
    "              validation_data=(x_test, y_test), \n",
    "              callbacks=[history,es])\n",
    "    t_stop = perf_counter()\n",
    "    t_diff = t_stop-t_start\n",
    "\n",
    "    #train_score = model.evaluate(x_train, y_train, verbose=0)\n",
    "    test_score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "    return test_score[0], test_score[1], t_diff, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the UCI-HAR features from DCT or DWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courtesy https://stackoverflow.com/questions/19233771/sklearn-plot-confusion-matrix-with-labels\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / np.sum(cm).astype('float')\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    f = plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    #plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uci_har_cwt_features(dataset, labels, waveletname):\n",
    "    data_shape = np.shape(dataset)\n",
    "    data_len = data_shape[0]\n",
    "    \n",
    "    # Now determine output shape to preallocate array for speed\n",
    "    coeff = get_cwt(dataset[0, :, 0], waveletname)\n",
    "    op_shape=[data_len]\n",
    "    op_shape.extend(list(np.shape(coeff)))\n",
    "    op_shape.append(data_shape[-1])\n",
    "    data_cwt = np.ndarray(shape=op_shape)\n",
    "\n",
    "    for ii in range(0,data_len):\n",
    "        if ii % 1000 == 0:\n",
    "            print(ii)\n",
    "        for jj in range(0,data_shape[-1]):\n",
    "            sig = dataset[ii, :, jj]\n",
    "            coeff = get_cwt(sig, waveletname)\n",
    "            #coeff_ = coeff[:,:127]\n",
    "            data_cwt[ii, :, :, jj] = coeff\n",
    "\n",
    "    X = np.array(data_cwt)\n",
    "    Y = labels\n",
    "    return X, Y\n",
    "\n",
    "def get_uci_har_stft_features(dataset, labels, M, noverlap, win):\n",
    "    data_shape = np.shape(dataset)\n",
    "    data_len = data_shape[0]\n",
    "\n",
    "    # Now determine output shape to preallocate array for speed\n",
    "    coeff = get_spectrogram(dataset[0, :, 0], M, noverlap, windowname = win)\n",
    "    op_shape=[data_len]\n",
    "    op_shape.extend(list(np.shape(coeff)))\n",
    "    op_shape.append(data_shape[-1])\n",
    "    data_stft = np.ndarray(shape=op_shape)\n",
    "\n",
    "    for ii in range(0,data_len):\n",
    "        if ii % 1000 == 0:\n",
    "            print(ii)\n",
    "        for jj in range(0,data_shape[-1]):\n",
    "            sig = dataset[ii, :, jj]\n",
    "            coeff = get_spectrogram(sig, M, noverlap, windowname = win)\n",
    "            #coeff_ = coeff[:,:127]\n",
    "            #import pdb; pdb.set_trace()\n",
    "            data_stft[ii, :, :, jj] = coeff\n",
    "\n",
    "    X = np.array(data_stft)\n",
    "    Y = labels\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we run the classifiers in a loop of different truncation lengths of the input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "labels_arr=list(activities_description[k] for k in activities_description.keys())\n",
    "\n",
    "fig_perf = plt.figure(figsize=(36,14))\n",
    "spec = gridspec.GridSpec(ncols=2, nrows=1)\n",
    "perf_left_ax = fig_perf.add_subplot(spec[0,0])\n",
    "perf_right_ax = fig_perf.add_subplot(spec[0,1])\n",
    "perf_left_ax.set_title('Accuracy',fontsize=26)\n",
    "perf_right_ax.set_title('Loss',fontsize=26)\n",
    "\n",
    "# for testing\n",
    "num_train=None\n",
    "num_test=None\n",
    "\n",
    "# STFT first\n",
    "x_train, y_train = get_uci_har_stft_features(uci_har_signals_train[:num_train,:,:], \n",
    "                                             uci_har_labels_train[:num_train], M, noverlap, win)\n",
    "x_test, y_test = get_uci_har_stft_features(uci_har_signals_test[:num_test,:,:], \n",
    "                                           uci_har_labels_test[:num_test], M, noverlap, win)\n",
    "val_loss, val_acc, train_time, mdl = NN(x_train, y_train, x_test, y_test)\n",
    "\n",
    "y_pred = np.argmax(mdl.predict(x_test),axis=1)\n",
    "cm = confusion_matrix(y_test,y_pred,labels=range(no_labels))\n",
    "plot_confusion_matrix(cm,labels_arr,title='Spectrogram')\n",
    "    \n",
    "plt.figure(fig_perf.number)\n",
    "perf_left_ax.plot(history.history['val_accuracy'], label='stft')\n",
    "perf_right_ax.plot(history.history['val_loss'], label='stft')\n",
    "\n",
    "# CWTs next\n",
    "#wavelets=['gaus1']\n",
    "for w in wavelets:\n",
    "    x_train, y_train = get_uci_har_cwt_features(uci_har_signals_train[:num_train,:,:],\n",
    "                                                uci_har_labels_train[:num_train], w)\n",
    "    x_test, y_test = get_uci_har_cwt_features(uci_har_signals_test[:num_test,:,:], \n",
    "                                              uci_har_labels_test[:num_test], w)\n",
    "    val_loss, val_acc, train_time, mdl = NN(x_train, y_train, x_test, y_test)\n",
    "\n",
    "    y_pred = np.argmax(mdl.predict(x_test),axis=1)\n",
    "    cm = confusion_matrix(y_test,y_pred,labels=range(no_labels))\n",
    "    plot_confusion_matrix(cm,labels_arr,title='CWT {}'.format(w))\n",
    "\n",
    "    plt.figure(fig_perf.number)\n",
    "    perf_left_ax.plot(history.history['val_accuracy'], label='cwt-{}'.format(w))\n",
    "    perf_right_ax.plot(history.history['val_loss'], label='cwt-{}'.format(w))\n",
    "\n",
    "plt.figure(fig_perf.number)\n",
    "perf_left_ax.set_ylabel('Accuracy', fontsize=22)\n",
    "perf_right_ax.set_ylabel('Loss', fontsize=22)\n",
    "perf_left_ax.set_xlabel('Epoch', fontsize=22)\n",
    "perf_right_ax.set_xlabel('Epoch', fontsize=22)\n",
    "perf_left_ax.legend(fontsize=18)\n",
    "perf_right_ax.legend(fontsize=18)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pywt.wavelist(kind='continuous'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
