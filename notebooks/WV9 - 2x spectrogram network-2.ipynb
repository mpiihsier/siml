{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introduction\n",
    "Given signal of length L, and the STFT parameters:\n",
    "1. Window length, $M$\n",
    "2. Shift/stride, $R$ ($ 1 \\le R \\le M $, for no loss of information)\n",
    "3. FFT size, $N$ ($N\\ge M$, for our purpose, $ N=M $)\n",
    "Then # segments, $K$, will be $ K=\\lfloor (L-M)/R \\rfloor$\n",
    "In our problem, our data samples have $L=128$, which limits our options for window length. If we choose a large $M$ necessary for finer resolution of the frequency components (say $M\\ge L$ with zero-padding or over-sampling), we would lose the temporal information of when the frequency peaks occur. So we will make the following tradeoff: $N=M=32$, $R=2$. Furthermore prefix and suffix $M/2$ samples to the signal frame to also fully incorporate the spectral behavior at the edges (when using tapered windows), thus $L'=128+N=160$. With these parameters and adjustments, $K=64$. Thus the inputs to our CNN classifier will be $(M/2+1)\\times K=17\\times 64$ spectrogram images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "import keras\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import History \n",
    "history = History()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading the UCI HAR dataset into an numpy ndarray\n",
    "Download dataset from https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train dataset contains 7352 signals, each one of length 128 and 9 components \n",
      "The test dataset contains 2947 signals, each one of length 128 and 9 components \n",
      "The train dataset contains 7352 labels, with the following distribution:\n",
      " Counter({5: 1407, 4: 1374, 3: 1286, 0: 1226, 1: 1073, 2: 986})\n",
      "The test dataset contains 2947 labels, with the following distribution:\n",
      " Counter({5: 537, 4: 532, 0: 496, 3: 491, 1: 471, 2: 420})\n"
     ]
    }
   ],
   "source": [
    "activities_description = {\n",
    "    1: 'walking',\n",
    "    2: 'walking upstairs',\n",
    "    3: 'walking downstairs',\n",
    "    4: 'sitting',\n",
    "    5: 'standing',\n",
    "    6: 'laying'\n",
    "}\n",
    "\n",
    "def read_signals(filename):\n",
    "    with open(filename, 'r') as fp:\n",
    "        data = fp.read().splitlines()\n",
    "        data = map(lambda x: x.strip().split(), data)\n",
    "        data = [list(map(float, line)) for line in data]\n",
    "    return data\n",
    "\n",
    "def read_labels(filename):        \n",
    "    with open(filename, 'r') as fp:\n",
    "        activities = fp.read().splitlines()\n",
    "        activities = list(map(lambda x: int(x)-1, activities))\n",
    "    return activities\n",
    "\n",
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation, :, :]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "\n",
    "DATA_FOLDER = '../../UCI HAR Dataset/'\n",
    "INPUT_FOLDER_TRAIN = DATA_FOLDER+'train/Inertial Signals/'\n",
    "INPUT_FOLDER_TEST = DATA_FOLDER+'test/Inertial Signals/'\n",
    "\n",
    "INPUT_FILES_TRAIN = ['body_acc_x_train.txt', 'body_acc_y_train.txt', 'body_acc_z_train.txt', \n",
    "                     'body_gyro_x_train.txt', 'body_gyro_y_train.txt', 'body_gyro_z_train.txt',\n",
    "                     'total_acc_x_train.txt', 'total_acc_y_train.txt', 'total_acc_z_train.txt']\n",
    "\n",
    "INPUT_FILES_TEST = ['body_acc_x_test.txt', 'body_acc_y_test.txt', 'body_acc_z_test.txt', \n",
    "                     'body_gyro_x_test.txt', 'body_gyro_y_test.txt', 'body_gyro_z_test.txt',\n",
    "                     'total_acc_x_test.txt', 'total_acc_y_test.txt', 'total_acc_z_test.txt']\n",
    "\n",
    "LABELFILE_TRAIN = DATA_FOLDER+'train/y_train.txt'\n",
    "LABELFILE_TEST = DATA_FOLDER+'test/y_test.txt'\n",
    "\n",
    "train_signals, test_signals = [], []\n",
    "\n",
    "for input_file in INPUT_FILES_TRAIN:\n",
    "    sig = read_signals(INPUT_FOLDER_TRAIN + input_file)\n",
    "    train_signals.append(sig)\n",
    "train_signals = np.transpose(train_signals, (1, 2, 0))\n",
    "\n",
    "for input_file in INPUT_FILES_TEST:\n",
    "    sig = read_signals(INPUT_FOLDER_TEST + input_file)\n",
    "    test_signals.append(sig)\n",
    "test_signals = np.transpose(test_signals, (1, 2, 0))\n",
    "\n",
    "train_labels = read_labels(LABELFILE_TRAIN)\n",
    "test_labels = read_labels(LABELFILE_TEST)\n",
    "\n",
    "[no_signals_train, no_steps_train, no_components_train] = np.shape(train_signals)\n",
    "[no_signals_test, no_steps_test, no_components_test] = np.shape(test_signals)\n",
    "no_labels = len(np.unique(train_labels[:]))\n",
    "\n",
    "print(\"The train dataset contains {} signals, each one of length {} and {} components \".format(no_signals_train, no_steps_train, no_components_train))\n",
    "print(\"The test dataset contains {} signals, each one of length {} and {} components \".format(no_signals_test, no_steps_test, no_components_test))\n",
    "print(\"The train dataset contains {} labels, with the following distribution:\\n {}\".format(np.shape(train_labels)[0], Counter(train_labels[:])))\n",
    "print(\"The test dataset contains {} labels, with the following distribution:\\n {}\".format(np.shape(test_labels)[0], Counter(test_labels[:])))\n",
    "\n",
    "uci_har_signals_train, uci_har_labels_train = randomize(train_signals, np.array(train_labels))\n",
    "uci_har_signals_test, uci_har_labels_test = randomize(test_signals, np.array(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Applying a STFT to UCI HAR signals and saving the resulting spectrogram into an numpy ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n"
     ]
    }
   ],
   "source": [
    "def plot_spectrogram(sig, M, noverlap, windowname = 'hann'):\n",
    "    # get the window taps\n",
    "    win = signal.get_window(windowname,M,False)\n",
    "    # prefix/suffix\n",
    "    pref = sig[-int(M/2):]*win[0:int(M/2)]\n",
    "    suf = sig[0:int(M/2)]*win[-int(M/2):]\n",
    "\n",
    "    sig = np.concatenate((pref, sig))\n",
    "    sig = np.concatenate((sig,suf))\n",
    "    f, t, Sxx = signal.spectrogram(sig, window=win, nperseg=M, noverlap=noverlap)\n",
    "\n",
    "    return f,t,Sxx\n",
    "\n",
    "M = 32\n",
    "noverlap = M-2\n",
    "\n",
    "train_size = np.shape(train_signals)[0]\n",
    "#import pdb; pdb.set_trace()\n",
    "train_data_stft = np.ndarray(shape=(train_size, 17, 65, 9*1))\n",
    "\n",
    "for ii in range(0,train_size):\n",
    "    if ii % 1000 == 0:\n",
    "        print(ii)\n",
    "    for jj in range(0,9):\n",
    "        sig = uci_har_signals_train[ii, :, jj]\n",
    "        f,t,Sxx1 = plot_spectrogram(sig, M, noverlap, windowname='hann')\n",
    "        f,t,Sxx2 = plot_spectrogram(sig, M, noverlap, windowname='boxcar')\n",
    "        train_data_stft[ii, :, :, jj] = np.minimum(Sxx1,Sxx2)\n",
    "        \n",
    "\n",
    "test_size = np.shape(test_signals)[0]\n",
    "test_data_stft = np.ndarray(shape=(test_size, 17, 65, 9*1))\n",
    "for ii in range(0,test_size):\n",
    "    if ii % 100 == 0:\n",
    "        print(ii)\n",
    "    for jj in range(0,9):\n",
    "        sig = uci_har_signals_test[ii, :, jj]\n",
    "        f,t,Sxx1 = plot_spectrogram(sig, M, noverlap, windowname='hann')\n",
    "        f,t,Sxx2 = plot_spectrogram(sig, M, noverlap, windowname='boxcar')\n",
    "        test_data_stft[ii, :, :, jj] = np.minimum(Sxx1,Sxx2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (7352, 17, 65, 9)\n",
      "7352 train samples\n",
      "2947 test samples\n"
     ]
    }
   ],
   "source": [
    "x_train = train_data_stft\n",
    "y_train = list(uci_har_labels_train[:train_size])\n",
    "x_test = test_data_stft\n",
    "y_test = list(uci_har_labels_test[:test_size])\n",
    "img_x = 17\n",
    "img_y = 65\n",
    "img_z = 9*1\n",
    "num_classes = 6\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 100\n",
    "\n",
    "# reshape the data into a 4D tensor - (sample_number, x_img_size, y_img_size, num_channels)\n",
    "# because the MNIST is greyscale, we only have a single channel - RGB colour images would have 3\n",
    "input_shape = (img_x, img_y, img_z)\n",
    "\n",
    "# convert the data to the right type\n",
    "#x_train = x_train.reshape(x_train.shape[0], img_x, img_y, img_z)\n",
    "#x_test = x_test.reshape(x_test.shape[0], img_x, img_y, img_z)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices - this is for use in the\n",
    "# categorical_crossentropy loss below\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "460/460 [==============================] - 8s 16ms/step - loss: 0.7867 - accuracy: 0.5885 - val_loss: 0.7936 - val_accuracy: 0.5565\n",
      "Epoch 2/100\n",
      "460/460 [==============================] - 8s 18ms/step - loss: 0.6494 - accuracy: 0.6249 - val_loss: 0.6816 - val_accuracy: 0.6491\n",
      "Epoch 3/100\n",
      "460/460 [==============================] - 7s 16ms/step - loss: 0.6532 - accuracy: 0.6253 - val_loss: 0.7080 - val_accuracy: 0.6352\n",
      "Epoch 4/100\n",
      "460/460 [==============================] - 7s 15ms/step - loss: 0.6211 - accuracy: 0.6359 - val_loss: 0.7869 - val_accuracy: 0.6271\n",
      "Epoch 5/100\n",
      "460/460 [==============================] - 7s 16ms/step - loss: 0.6271 - accuracy: 0.6499 - val_loss: 0.9855 - val_accuracy: 0.6223\n",
      "Epoch 6/100\n",
      "460/460 [==============================] - 7s 16ms/step - loss: 0.6205 - accuracy: 0.6551 - val_loss: 0.7018 - val_accuracy: 0.6627\n",
      "Epoch 7/100\n",
      "403/460 [=========================>....] - ETA: 0s - loss: 0.5955 - accuracy: 0.6771"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1), activation='relu', input_shape=input_shape)) \n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "              optimizer=keras.optimizers.Adam(), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, \n",
    "          epochs=epochs, verbose=1, \n",
    "          validation_data=(x_test, y_test), \n",
    "          callbacks=[history])\n",
    "\n",
    "train_score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print('Train loss: {}, Train accuracy: {}'.format(train_score[0], train_score[1]))\n",
    "test_score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss: {}, Test accuracy: {}'.format(test_score[0], test_score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(figsize=(12,6), ncols=2)\n",
    "axarr[0].plot(range(1, epochs+1), history.history['accuracy'], label='train score')\n",
    "axarr[0].plot(range(1, epochs+1), history.history['val_accuracy'], label='test score')\n",
    "axarr[0].set_xlabel('Number of Epochs', fontsize=18)\n",
    "axarr[0].set_ylabel('Accuracy', fontsize=18)\n",
    "axarr[0].set_ylim([0,1])\n",
    "axarr[1].plot(range(1, epochs+1), history.history['accuracy'], label='train score')\n",
    "axarr[1].plot(range(1, epochs+1), history.history['val_accuracy'], label='test score')\n",
    "axarr[1].set_xlabel('Number of Epochs', fontsize=18)\n",
    "axarr[1].set_ylabel('Accuracy', fontsize=18)\n",
    "axarr[1].set_ylim([0.6,1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
