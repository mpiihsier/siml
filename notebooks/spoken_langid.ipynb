{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from time import perf_counter\n",
    "import numpy as np\n",
    "\n",
    "import soundfile as sf\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Conv1D, BatchNormalization, Dense, Flatten, Activation\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import History, EarlyStopping\n",
    "history = History()\n",
    "\n",
    "#data_normalized=False\n",
    "generator_init = False\n",
    "model_saved = False\n",
    "SAVED_MODEL_PATH = '../models/'\n",
    "MODEL_NAME = SAVED_MODEL_PATH+'langid_model'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset from https://www.kaggle.com/toponowicz/spoken-language-identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator for serving up batches for training the NN\n",
    "# ref https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "from keras.utils import Sequence\n",
    "import pywt\n",
    "\n",
    "class langidDataGenerator(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, wavelet='rbio3.1', truncate_len=None,\n",
    "                 batch_size=32, n_channels=1, n_classes=6, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.wvlt = wavelet\n",
    "        self.trunc_len = truncate_len\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        # determine dimensions of input\n",
    "        #import pdb; pdb.set_trace()\n",
    "        X,y=self.__data_generation(self.list_IDs[:2])\n",
    "        X = np.expand_dims(X,2)\n",
    "        self.dim = X.shape[1:]\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        X = np.expand_dims(X,2)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def wavelet_features(self, list_IDs_temp):\n",
    "        features = []\n",
    "        y=[]\n",
    "        # Take the DWT of each component and concat them end-to-end\n",
    "        for ID in list_IDs_temp:\n",
    "            # Store sample\n",
    "            signal,fs = sf.read(ID)\n",
    "            list_coeff = pywt.wavedec(signal, self.wvlt, mode='per')\n",
    "            # string the coefficient arrays end-end to keep like ones together\n",
    "            dwt_local_coeff=[]\n",
    "            end_flag=0\n",
    "            for coeff in list_coeff:\n",
    "                if not end_flag:\n",
    "                    dwt_local_coeff.extend(coeff)\n",
    "                    flag=1\n",
    "                else:\n",
    "                    flag=0\n",
    "                    dwt_local_coeff.extend(coeff.reverse())\n",
    "            features.append(dwt_local_coeff[:self.trunc_len])\n",
    "            y.append(self.labels[ID])\n",
    "        X = np.array(features)\n",
    "        return X,y\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        X,y = self.wavelet_features(list_IDs_temp)\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = '../datasets/langid/'\n",
    "train_path = DATA_FOLDER+'/train/'\n",
    "test_path = DATA_FOLDER+'/test/'\n",
    "\n",
    "#para train path\n",
    "train_labels = []\n",
    "for filename in os.listdir(train_path):\n",
    "    train_labels.append(filename[:4]) # [:4] file naming convention, f.e. es_f\n",
    "test_labels = []\n",
    "for filename in os.listdir(test_path):\n",
    "    test_labels.append(filename[:4])\n",
    "\n",
    "# Create numeric labels\n",
    "lb=0\n",
    "labeld={}\n",
    "for k in Counter(train_labels).keys():\n",
    "    labeld[k] = lb\n",
    "    lb=lb +1\n",
    "# number of classes\n",
    "num_classes = lb\n",
    "\n",
    "# Create list of training/test filenames and dict {filename : label}\n",
    "train_files = []\n",
    "train_labels2={}\n",
    "for filename in os.listdir(train_path):\n",
    "    train_files.append(train_path+filename)\n",
    "    train_labels2[train_path+filename] = labeld[filename[:4]]\n",
    "\n",
    "test_files = []\n",
    "test_labels2={}\n",
    "for filename in os.listdir(test_path):\n",
    "    test_files.append(test_path+filename)\n",
    "    test_labels2[test_path+filename] = labeld[filename[:4]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEarlyStopping(keras.callbacks.Callback):\n",
    "    def __init__(self, patience=0):\n",
    "        super(CustomEarlyStopping, self).__init__()\n",
    "        self.patience = patience\n",
    "        self.best_weights = None\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best_v_loss = np.Inf\n",
    "        self.best_v_accuracy = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None): \n",
    "        v_loss=logs.get('val_loss')\n",
    "        v_acc=logs.get('val_accuracy')\n",
    "\n",
    "        # If BOTH the val_loss AND val_accuracy do not improve for 'patience' epochs, \n",
    "        # stop training early.\n",
    "        if np.less(v_loss, self.best_v_loss) or np.greater(v_acc, self.best_v_accuracy):\n",
    "            self.best_v_loss = v_loss\n",
    "            self.best_v_accuracy = v_acc\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "                \n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization (BatchNo (None, 16384, 1)          4         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 5459, 32)          320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 5459, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 5459, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 2727, 16)          3600      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 2727, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2727, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 2723, 16)          1296      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 2723, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2723, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 2721, 8)           392       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 2721, 8)           32        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2721, 8)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2721, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 2721, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2721, 1024)        0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2721, 256)         262400    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 2721, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2721, 256)         0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2721, 64)          16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 2721, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 2721, 64)          0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2721, 16)          1040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 2721, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2721, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 43536)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 261222    \n",
      "=================================================================\n",
      "Total params: 561,666\n",
      "Trainable params: 558,800\n",
      "Non-trainable params: 2,866\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From <ipython-input-5-5d63a7342606>:66: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/128\n"
     ]
    }
   ],
   "source": [
    "epochs = 128\n",
    "no_train=None\n",
    "no_test=None\n",
    "\n",
    "# Parameters\n",
    "params = {'wavelet' :'bior6.8',\n",
    "          'truncate_len' : 16*1024,\n",
    "          'batch_size' : 1024,\n",
    "          'n_classes' : 6,\n",
    "          'n_channels' : 1,\n",
    "          'shuffle' : True}\n",
    "\n",
    "if not generator_init:\n",
    "    # Generators\n",
    "    training_generator = langidDataGenerator(train_files[:no_train], train_labels2, **params)\n",
    "    validation_generator = langidDataGenerator(test_files[:no_test], test_labels2, **params)\n",
    "\"\"\"\n",
    "if not data_normalized:\n",
    "    training_generator.normalize_data()\n",
    "\"\"\"\n",
    "\n",
    "if not model_saved:\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_shape=training_generator.dim))\n",
    "    model.add(Conv1D(32, kernel_size=9, strides=3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D(16, kernel_size=7, strides=2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D(16, kernel_size=5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D(8, kernel_size=3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1024, kernel_regularizer=keras.regularizers.l1_l2(l1=5e-5,l2=5e-5)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(256, kernel_regularizer=keras.regularizers.l1_l2(l1=5e-4,l2=5e-4)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(64, kernel_regularizer=keras.regularizers.l1_l2(l1=5e-5,l2=5e-5)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(16, kernel_regularizer=keras.regularizers.l1_l2(l1=1e-5,l2=1e-5)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, kernel_regularizer=keras.regularizers.l1_l2(l1=5e-5,l2=5e-5),\\\n",
    "                    activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=keras.optimizers.Adam(), \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "else:\n",
    "    model = keras.models.load_model(MODEL_NAME)\n",
    "\n",
    "es = CustomEarlyStopping(patience=8)\n",
    "\n",
    "t_start = perf_counter()\n",
    "# Train model on dataset\n",
    "model.fit_generator(generator=training_generator,\n",
    "                    validation_data=validation_generator, epochs=epochs,\n",
    "                    use_multiprocessing=False, verbose=1,\n",
    "                    workers=0, callbacks=[history,es])\n",
    "\"\"\"\n",
    "model.fit(training_generator, epochs=epochs, verbose=1,\n",
    "          callbacks=[history,es])\n",
    "\"\"\"\n",
    "t_stop = perf_counter()\n",
    "\n",
    "model.save(MODEL_NAME)\n",
    "\n",
    "t_diff = t_stop-t_start\n",
    "print ('Time to train the network {} seconds'.format(t_diff))\n",
    "\n",
    "train_score = model.evaluate(training_generator, verbose=0)\n",
    "print('Train loss: {}, Train accuracy: {}'.format(train_score[0], train_score[1]))\n",
    "test_score = model.evaluate(validation_generator, verbose=0)\n",
    "print('Test loss: {}, Test accuracy: {}'.format(test_score[0], test_score[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(figsize=(14,7), ncols=2)\n",
    "axarr[0].plot(history.history['accuracy'], label='train accuracy')\n",
    "axarr[0].plot(history.history['val_accuracy'], label='test accuracy')\n",
    "axarr[0].set_xlabel('Number of Epochs', fontsize=18)\n",
    "axarr[0].set_ylabel('Accuracy', fontsize=18)\n",
    "axarr[0].set_ylim([0.1,1])\n",
    "axarr[0].legend()\n",
    "\n",
    "axarr[1].plot(history.history['loss'], label='train loss')\n",
    "axarr[1].plot(history.history['val_loss'], label='test loss')\n",
    "axarr[1].set_xlabel('Number of Epochs', fontsize=18)\n",
    "axarr[1].set_ylabel('Loss', fontsize=18)\n",
    "axarr[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pywt.wavelist(kind='discrete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
