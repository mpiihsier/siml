{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from time import perf_counter\n",
    "import numpy as np\n",
    "\n",
    "import soundfile as sf\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Conv1D, BatchNormalization, Dense, Flatten, Activation\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import History, EarlyStopping\n",
    "history = History()\n",
    "\n",
    "#data_normalized=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset from https://www.kaggle.com/toponowicz/spoken-language-identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator for serving up batches for training the NN\n",
    "# ref https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "from keras.utils import Sequence\n",
    "import pywt\n",
    "\n",
    "class langidDataGenerator(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, wavelet='rbio3.1', truncate_len=None,\n",
    "                 batch_size=32, n_channels=1, n_classes=6, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.wvlt = wavelet\n",
    "        self.trunc_len = truncate_len\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        # determine dimensions of input\n",
    "        X,y=self.__getitem__(0)\n",
    "        #import pdb; pdb.set_trace()\n",
    "        self.dim = X.shape[1:]\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        X = np.expand_dims(X,2)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def wavelet_features(self, list_IDs_temp):\n",
    "        features = []\n",
    "        y=[]\n",
    "        # Take the DWT of each component and concat them end-to-end\n",
    "        for ID in list_IDs_temp:\n",
    "            # Store sample\n",
    "            signal,fs = sf.read(ID)\n",
    "            list_coeff = pywt.wavedec(signal, self.wvlt, mode='per')\n",
    "            # string the coefficient arrays end-end to keep like ones together\n",
    "            dwt_local_coeff=[]\n",
    "            end_flag=0\n",
    "            for coeff in list_coeff:\n",
    "                if not end_flag:\n",
    "                    dwt_local_coeff.extend(coeff)\n",
    "                    flag=1\n",
    "                else:\n",
    "                    flag=0\n",
    "                    dwt_local_coeff.extend(coeff.reverse())\n",
    "            features.append(dwt_local_coeff[:self.trunc_len])\n",
    "            y.append(self.labels[ID])\n",
    "        X = np.array(features)\n",
    "        return X,y\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        X,y = self.wavelet_features(list_IDs_temp)\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = '../datasets/langid/'\n",
    "train_path = DATA_FOLDER+'/train/'\n",
    "test_path = DATA_FOLDER+'/test/'\n",
    "\n",
    "#para train path\n",
    "train_labels = []\n",
    "for filename in os.listdir(train_path):\n",
    "    train_labels.append(filename[:4]) # [:4] file naming convention, f.e. es_f\n",
    "test_labels = []\n",
    "for filename in os.listdir(test_path):\n",
    "    test_labels.append(filename[:4])\n",
    "\n",
    "# Create numeric labels\n",
    "lb=0\n",
    "labeld={}\n",
    "for k in Counter(train_labels).keys():\n",
    "    labeld[k] = lb\n",
    "    lb=lb +1\n",
    "# number of classes\n",
    "num_classes = lb\n",
    "\n",
    "# Create list of training/test filenames and dict {filename : label}\n",
    "train_files = []\n",
    "train_labels2={}\n",
    "for filename in os.listdir(train_path):\n",
    "    train_files.append(train_path+filename)\n",
    "    train_labels2[train_path+filename] = labeld[filename[:4]]\n",
    "\n",
    "test_files = []\n",
    "test_labels2={}\n",
    "for filename in os.listdir(test_path):\n",
    "    test_files.append(test_path+filename)\n",
    "    test_labels2[test_path+filename] = labeld[filename[:4]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 339, 32)           320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 339, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 339, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 167, 16)           3600      \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 167, 16)           64        \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 167, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 163, 16)           1296      \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 163, 16)           64        \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 163, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 161, 8)            392       \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 161, 8)            32        \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 161, 8)            0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 161, 128)          1152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 161, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 161, 128)          0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 161, 96)           12384     \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 161, 96)           384       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 161, 96)           0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 161, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 161, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 161, 64)           0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 161, 16)           1040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 161, 16)           64        \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 161, 16)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2576)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 6)                 15462     \n",
      "=================================================================\n",
      "Total params: 43,358\n",
      "Trainable params: 42,606\n",
      "Non-trainable params: 752\n",
      "_________________________________________________________________\n",
      "Epoch 1/128\n",
      "31/31 [==============================] - 71s 2s/step - loss: 2.5757 - accuracy: 0.1986 - val_loss: 2.3980 - val_accuracy: 0.1771\n",
      "Epoch 2/128\n",
      "31/31 [==============================] - 57s 2s/step - loss: 2.3967 - accuracy: 0.2308 - val_loss: 2.3831 - val_accuracy: 0.1562\n",
      "Epoch 3/128\n",
      "31/31 [==============================] - 53s 2s/step - loss: 2.3277 - accuracy: 0.2611 - val_loss: 2.3670 - val_accuracy: 0.1875\n",
      "Epoch 4/128\n",
      "31/31 [==============================] - 68s 2s/step - loss: 2.2335 - accuracy: 0.2893 - val_loss: 2.4070 - val_accuracy: 0.1979\n",
      "Epoch 5/128\n",
      "31/31 [==============================] - 66s 2s/step - loss: 2.1605 - accuracy: 0.3170 - val_loss: 2.4327 - val_accuracy: 0.1354\n",
      "Epoch 6/128\n",
      "31/31 [==============================] - 66s 2s/step - loss: 2.0941 - accuracy: 0.3407 - val_loss: 2.4423 - val_accuracy: 0.2135\n",
      "Epoch 7/128\n",
      "31/31 [==============================] - 69s 2s/step - loss: 2.0378 - accuracy: 0.3649 - val_loss: 2.5540 - val_accuracy: 0.2188\n",
      "Epoch 8/128\n",
      "31/31 [==============================] - 69s 2s/step - loss: 1.9536 - accuracy: 0.3947 - val_loss: 2.4274 - val_accuracy: 0.2188\n",
      "Epoch 9/128\n",
      "31/31 [==============================] - 62s 2s/step - loss: 1.8918 - accuracy: 0.4400 - val_loss: 2.4491 - val_accuracy: 0.2656\n",
      "Epoch 10/128\n",
      "31/31 [==============================] - 78s 3s/step - loss: 1.7968 - accuracy: 0.4612 - val_loss: 2.4069 - val_accuracy: 0.2344\n",
      "Epoch 11/128\n",
      "31/31 [==============================] - 72s 2s/step - loss: 1.7296 - accuracy: 0.4929 - val_loss: 2.5645 - val_accuracy: 0.1875\n",
      "Time to train the network 755.983413128 seconds\n",
      "Train loss: 2.4654462337493896, Train accuracy: 0.20614919066429138\n",
      "Test loss: 2.5517289638519287, Test accuracy: 0.1822916716337204\n"
     ]
    }
   ],
   "source": [
    "epochs = 128\n",
    "no_train=2000\n",
    "no_test=200\n",
    "\n",
    "# Parameters\n",
    "params = {'wavelet' :'rbio3.1',\n",
    "          'truncate_len' : 1024,\n",
    "          'batch_size' : 64,\n",
    "          'n_classes' : 6,\n",
    "          'n_channels' : 1,\n",
    "          'shuffle' : True}\n",
    "\n",
    "# Generators\n",
    "training_generator = langidDataGenerator(train_files[:no_train], train_labels2, **params)\n",
    "validation_generator = langidDataGenerator(test_files[:no_test], test_labels2, **params)\n",
    "\"\"\"\n",
    "if not data_normalized:\n",
    "    training_generator.normalize_data()\n",
    "\"\"\"\n",
    "model = Sequential()\n",
    "model.add(Conv1D(32, kernel_size=9, strides=3, input_shape=training_generator.dim))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(16, kernel_size=7, strides=2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(16, kernel_size=5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(8, kernel_size=3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(128, kernel_regularizer=keras.regularizers.l1_l2(l1=5e-5,l2=5e-5)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(96, kernel_regularizer=keras.regularizers.l1_l2(l1=5e-4,l2=5e-4)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(64, kernel_regularizer=keras.regularizers.l1_l2(l1=5e-5,l2=5e-5)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16, kernel_regularizer=keras.regularizers.l1_l2(l1=1e-5,l2=1e-5)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, kernel_regularizer=keras.regularizers.l1_l2(l1=5e-5,l2=5e-5),\\\n",
    "                activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "              optimizer=keras.optimizers.Adam(), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='loss', verbose=0, patience=8)\n",
    "t_start = perf_counter()\n",
    "# Train model on dataset\n",
    "model.fit_generator(generator=training_generator,\n",
    "                    validation_data=validation_generator, epochs=epochs,\n",
    "                    use_multiprocessing=False, verbose=1,\n",
    "                    workers=0, callbacks=[history,es])\n",
    "\"\"\"\n",
    "model.fit(training_generator, epochs=epochs, verbose=1,\n",
    "          callbacks=[history,es])\n",
    "\"\"\"\n",
    "\n",
    "t_stop = perf_counter()\n",
    "t_diff = t_stop-t_start\n",
    "print ('Time to train the network {} seconds'.format(t_diff))\n",
    "\n",
    "train_score = model.evaluate(training_generator, verbose=0)\n",
    "print('Train loss: {}, Train accuracy: {}'.format(train_score[0], train_score[1]))\n",
    "test_score = model.evaluate(validation_generator, verbose=0)\n",
    "print('Test loss: {}, Test accuracy: {}'.format(test_score[0], test_score[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(figsize=(14,7), ncols=2)\n",
    "axarr[0].plot(history.history['accuracy'], label='train accuracy')\n",
    "axarr[0].plot(history.history['val_accuracy'], label='test accuracy')\n",
    "axarr[0].set_xlabel('Number of Epochs', fontsize=18)\n",
    "axarr[0].set_ylabel('Accuracy', fontsize=18)\n",
    "axarr[0].set_ylim([0.5,1])\n",
    "axarr[0].legend()\n",
    "\n",
    "axarr[1].plot(history.history['loss'], label='train loss')\n",
    "axarr[1].plot(history.history['val_loss'], label='test loss')\n",
    "axarr[1].set_xlabel('Number of Epochs', fontsize=18)\n",
    "axarr[1].set_ylabel('Loss', fontsize=18)\n",
    "axarr[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
